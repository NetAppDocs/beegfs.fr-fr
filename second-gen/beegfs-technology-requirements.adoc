---
sidebar: sidebar 
permalink: second-gen/beegfs-technology-requirements.html 
keywords: BeeGFS on NetApp, NetApp Verified Architecture, EF600 
summary: Pour implémenter la solution BeeGFS sur NetApp, assurez-vous que votre environnement répond aux exigences technologiques. 
---
= Exigences techniques
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Pour implémenter la solution BeeGFS sur NetApp, assurez-vous que votre environnement répond aux exigences technologiques indiquées dans ce document.



== Configuration matérielle requise

Avant de commencer, assurez-vous que votre matériel répond aux spécifications suivantes pour un design modulaire deuxième génération de la solution BeeGFS sur NetApp. Les composants exacts d'un déploiement particulier peuvent varier en fonction des besoins du client.

[cols="10%,20%,70%"]
|===
| Quantité | Composant matériel | De formation 


 a| 
2
 a| 
Nœuds de fichiers BeeGFS
 a| 
Pour atteindre les performances attendues, chaque nœud de fichier doit satisfaire ou dépasser les spécifications des nœuds de fichiers recommandés.

*Options de noeud de fichier recommandées :*

* *Lenovo ThinkSystem SR665 V3*
+
** *Processeurs:* 2x AMD EPYC 9124 16C 3.0 GHz (configurés comme deux zones NUMA).
** *Mémoire :* 256 Go (16 x 16 Go TruDDR5 4800 MHz RDIMM-A)
** *Extension PCIe :* quatre emplacements PCIe Gen5 x16 (deux par zone NUMA)
** *Divers:*
+
*** Deux disques en RAID 1 pour le système d'exploitation (1 To 7200 tr/min SATA)
*** Port 1 GbE pour la gestion du système d'exploitation intrabande
*** BMC 1GbE avec API Redfish pour la gestion des serveurs hors bande
*** Deux blocs d'alimentation remplaçables à chaud et ventilateurs haute performance




* *Lenovo ThinkSystem SR665*
+
** *Processeurs:* 2x AMD EPYC 7343 16C 3.2 GHz (configurés comme deux zones NUMA).
** *Mémoire :* 256 Go (16 x 16 Go TruDDR4 3200 MHz RDIMM-A)
** *Extension PCIe :* quatre emplacements PCIe Gen4 x16 (deux par zone NUMA)
** *Divers:*
+
*** Deux disques en RAID 1 pour le système d'exploitation (1 To 7200 tr/min SATA)
*** Port 1 GbE pour la gestion du système d'exploitation intrabande
*** BMC 1GbE avec API Redfish pour la gestion des serveurs hors bande
*** Deux blocs d'alimentation remplaçables à chaud et ventilateurs haute performance








| 2 | Nœuds de bloc E-Series (baie EF600)  a| 
*Mémoire :* 256 Go (128 Go par contrôleur). *Adaptateur :* 2 ports 200 Go/HDR (NVMe/IB). *Lecteurs :* configurés pour correspondre aux métadonnées et à la capacité de stockage souhaitées.



| 8 | Adaptateurs de carte hôte InfiniBand (pour les nœuds de fichiers).  a| 
Les adaptateurs de carte hôte varient en fonction du modèle de serveur utilisé pour le nœud de fichiers. Recommandations pour les nœuds de fichiers vérifiés :

* *Lenovo ThinkSystem SR665 V3 Server:*
+
** MCX755106AS-HEAT ConnectX-7, NDR200, QSFP112, 2 ports, PCIe Gen5 x16, adaptateur InfiniBand


* *Lenovo ThinkSystem SR665 Server:*
+
** MCX653106A-HDAT ConnectX-6, HDR, QSFP-56, 2 ports, PCIe Gen4 x16, adaptateur InfiniBand






| 1 | Switch réseau de stockage  a| 
Le commutateur du réseau de stockage doit offrir une vitesse InfiniBand 200 Gbit/s. Modèles de commutateurs recommandés :

* *Commutateur NVIDIA QM9700 Quantum 2 NDR InfiniBand*
* *Commutateur NVIDIA MQM8700 Quantum HDR InfiniBand*


|===


=== Exigences de câblage

*Connexions directes des nœuds de bloc aux nœuds de fichier.*

[cols="10%,70%,20%"]
|===
| Quantité | Référence | Longueur 


| 8 | MCP1650-H001E30 (câble en cuivre passif NVIDIA, QSFP56, 200 Gbit/s) | 1 m 
|===
*Connexions entre les nœuds de fichiers et le commutateur de réseau de stockage.* Sélectionnez l'option de câble appropriée dans le tableau suivant en fonction de votre commutateur de stockage InfiniBand. + la longueur de câble recommandée est de 2 M. toutefois, elle peut varier en fonction de l'environnement du client.

[cols="20%,10%,15%,55%"]
|===
| Changer de modèle | Quantité | Type de câble | Référence 


| NVIDIA QM9700 | 4 | Fibre active | MFA7U10-H002 (câble fibre actif NVIDIA, InfiniBand 400 Go/s à 2 200 Go/s, OSFP à 2 QSFP56) 


| NVIDIA QM9700 | 4 | Cuivre passif | MCP7Y60-H002 (câble en cuivre passif NVIDIA, InfiniBand 400 Go/s à 2 200 Go/s, OSFP à 2 QSFP56) 


| NVIDIA MQM8700 | 8 | Fibre active | MFS1S00-H003E (câble fibre active NVIDIA, InfiniBand 200 Gbit/s, QSFP56) 


| NVIDIA MQM8700 | 8 | Cuivre passif | MCP1650-H002E26 (câble en cuivre passif NVIDIA, InfiniBand 200 Gbit/s, QSFP56) 
|===


== Configuration logicielle requise

Pour des performances et une fiabilité prévisibles, les versions de la solution BeeGFS sur NetApp sont testées avec des versions spécifiques des composants logiciels requis pour implémenter la solution.



=== Configuration requise pour les nœuds de fichiers

[cols="20%,80%"]
|===
| Logiciel | Version 


 a| 
Red Hat Enterprise Linux
 a| 
Red Hat 9.3 Server Physical avec haute disponibilité (2 sockets).


IMPORTANT: Les nœuds de fichiers nécessitent un abonnement Red Hat Enterprise Linux Server valide et le module complémentaire haute disponibilité de Red Hat Enterprise Linux.



| Noyau Linux | 5.14.0-362.24.1.el9_3.x86_64 


| Pilotes InfiniBand / RDMA | MLNX_OFED_LINUX-23.10-3.2.2.0-LTS 


 a| 
Micrologiciel HCA
 a| 
*Firmware HCA ConnectX-7* FW: 28.39.1002 + PXE: 3.7.0201 + UEFI: 14.32.0012

*Firmware HCA ConnectX-6* FW: 20.31.1014 + PXE: 3.6.0403 + UEFI: 14.24.0013

|===


=== Exigences liées aux nœuds en mode bloc EF600

[cols="20%,80%"]
|===
| Logiciel | Version 


| SANtricity OS | 11.80.0 


| NVSRAM | N6000-880834-D08.dlp 


| Micrologiciel de lecteur | Dernière version disponible pour les modèles de lecteurs utilisés. 
|===


=== Configuration requise pour le déploiement de logiciels

Le tableau suivant répertorie les exigences logicielles déployées automatiquement dans le cadre du déploiement BeeGFS basé sur Ansible.

[cols="20%,80%"]
|===
| Logiciel | Version 


| BeeGFS | 7.4.4 


| Corosync | 3.1.5-4 


| Stimulateur cardiaque | 2.1.4-5 


| OpenSM  a| 
Openmm-5.17.2 (de MLNX_OFED_LINUX-23.10-3.2.2.0-LTS)

|===


=== Configuration requise pour le nœud de contrôle Ansible

La solution BeeGFS sur NetApp est déployée et gérée à partir d'un nœud de contrôle Ansible. Pour plus d'informations, reportez-vous à la section https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html["Documentation Ansible"^].

Les exigences logicielles répertoriées dans les tableaux suivants sont spécifiques à la version de la collection NetApp BeeGFS Ansible indiquée ci-dessous.

[cols="30%,70%"]
|===
| Logiciel | Version 


| Ansible | 6.x lorsqu'il est installé via pip : ansible-6.0.0 et ansible-core >= 2.13.0 


| Python | 3.9 (ou ultérieure) 


| Packs Python supplémentaires | Cryptographie-43.0.0, netaddr-1.3.0, ipaddr-2.2.0 


| Collection Ansible NetApp E-Series BeeGFS | 3.2.0 
|===